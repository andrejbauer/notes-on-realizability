\documentclass[a4paper,10pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{theorem}
\usepackage{listings}
\usepackage{url}
\usepackage{pdfsync}

%%% Settings for listings
\lstset{basicstyle=\ttfamily\small,xleftmargin=\parindent,language=Haskell}

\newcommand{\cc}[1]{\lstinline{#1}}

%%% Theorems
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

{\theorembodyfont{\rmfamily}
  \newtheorem{exercise}[theorem]{Exercise}
  \newtheorem{definition}[theorem]{Definition}
}

\newenvironment{proof}{\par\noindent\textit{Proof.}}{\hfill$\Box$\par\medskip}

%%% Macros

\newcommand{\RR}{\mathbb{R}}

\newcommand{\set}[1]{\{#1\}}
\newcommand{\such}{\mid}
\newcommand{\tpl}[1]{\mathcal{O}(#1)}
\newcommand{\cont}[1]{\mathcal{C}(#1)}
\newcommand{\scr}[1]{\mathcal{#1}}
\newcommand{\two}{\mathsf{2}}
\newcommand{\Bool}{\mathtt{Bool}}

\newcommand{\R}[1]{\mathtt{#1}}
\newcommand{\rz}{\Vdash}

\newcommand{\bind}{\mathbin{\text{\texttt{\char62\char62\char61}}}}

%%% Quantifiers

%%% quantifiers
\newcommand{\all}[3]{\forall\, #1 \,{\in}\, #2\,.\left(#3\right)}
\newcommand{\some}[3]{\exists\, #1 \,{\in}\, #2\,.\left(#3\right)}
\newcommand{\exactlyone}[3]{\exists!\, #1 \,{\in}\, #2\,.\left(#3\right)}
\newcommand{\lam}[3]{\lambda #1 \,{\in}\, #2\,.\left(#3\right)}
\newcommand{\uall}[2]{\forall\, #1\,.\left(#2\right)}
\newcommand{\usome}[2]{\exists\, #1\,.\left(#2\right)}
\newcommand{\uexactlyone}[3]{\exists!\, #1\,.\left(#2\right)}
\newcommand{\ulam}[2]{\lambda #1 .\left(#2\right)}
\newcommand{\xall}[3]{\forall\, #1 \,{\in}\, #2\,.\,#3}
\newcommand{\xsome}[3]{\exists\, #1 \,{\in}\, #2\,.\,#3}
\newcommand{\xexactlyone}[3]{\exists!\, #1 \,{\in}\, #2\,.\,#3}
\newcommand{\xuall}[2]{\forall\, #1\,.\,#2}
\newcommand{\xusome}[2]{\exists\, #1\,.\,#2}
\newcommand{\xuexactlyone}[2]{\exists!\, #1,.\,#2}
\newcommand{\xlam}[3]{\lambda #1 \,{\in}\, #2\,.\,#3}
\newcommand{\xulam}[2]{\lambda #1 .\,#2}
\newcommand{\tlam}[3]{\lambda #1 \,{:}\, #2\,.\,\left(#3\right)}
\newcommand{\xtlam}[3]{\lambda #1 \,{:}\, #2\,.\,#3}


\begin{document}

\title{Computation over compact and overt spaces}
\author{Andrej Bauer}

\maketitle

\begin{abstract}
    These are lecture notes for the graduate course ``Topology in computer
    science'' which I gave with Neža Mramor-Kosta in the Fall of~2009 at the
    Faculty of computer science, University of Ljubljana.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

When students first see the definition of compact spaces it usually looks very
mysterious to them. Where does the Heine-Borel property come from? Why would
anyone consider finite subcovers of arbitrary open covers? In these notes we
review the basic definitions and try to motivate them from the point of view
of computation.

We assume basic familiarity with topology and a certain degree of programming
skill. The code examples are written in Haskell, but could be translated to
any other programming language that supports higher-order functions.

These notes do not contain any original material. All is ``standard''
knowledge in topology and computation, although some parts are not well known.
A good starting point for background reading are Martín Escardó's
notes~\cite{escardo04:_synth}.

\section{Open sets as semidecidable properties}
\label{sec:opens-semidecidable}

An open subset of a topological space may be thought of as a property that can
be verified in finite time. To illustrate this, suppose we a real number $a$
is given to us in a physically realistic way. That is, $a$ is not given with
infinite precision all at once, but rather as a sequence of rational
approximations with error bounds. The approximations and the error bounds
might come from a sequence of ever more precise (and ever more expensive)
measurements, or from a computer program that computes~$a$ to any desired
precision.

We may want to know whether $a$ has a given property, or equivalently
whether it is an element of a given set $S \subseteq \RR$. We
distinguish between
% 
\begin{itemize}
\item \emph{deciding} whether $a \in S$: we do not know whether $a \in
  S$ or $a \not\in S$ and would like to know which is the case.
\item \emph{verifying} that $a \in S$: we seek evidence that $a \in S$
  (and if $a \not\in S$ then we do not care what happens, and in
  particular we do not have to provide evidence that $a \not\in S$).
\end{itemize}
% 
Deciding whether $a \in S$ provides one bit of information: we find out either
$a \in S$ or $a \not\in S$. Verifying that $a \in S$ provides \emph{less} than
that: if $a \in S$ we eventually get the evidence, but if $a \not\in S$ we
just wait forever hoping that perhaps the evidence of $a \in S$ will appear.

Can we decide whether $a > 0$? No, not for all $a$. The problematic case is $a
= 0$ because we will never be able to tell that $a > 0$ does not hold just by
looking at an approximation $a'$ with a positive error bound $\epsilon > 0$.

Can we verify that $a > 0$? Yes. If it is the case that $a > 0$ then
eventually we will see an approximation $a'$ with a good enough error bound
$\epsilon > 0$ such that $0 < a' - \epsilon < a$. If $a < 0$ then we do not
care what happens. (If you are thinking that we can decide whether $a > 0$ in
all cases but $a = 0$ then you are right. Of course, we are now talking about
a different topological space $\RR \setminus \set{0}$.)

A moment's thought shows that we can verify $a \in S$ when $S$ has the
following property: if $b \in S$ then also all points close enough to~$b$ are
in $S$. But that's precisely the definition of open sets. So we adopt the
slogan
% 
\begin{quote}
  \emph{``Verifiable properties correspond to open sets.''}
\end{quote}
% 
The word ``verifiable'' should be understood either as ``evidence can be
computed in finite time'' or ``an experiment confirming it can be performed''.

Now suppose $f : X \to Y$ is a map between topological spaces and $U \subseteq
Y$ is open (verifiable). Imagine we can implement $f$ on a computer or as a
physical process. For every $x \in X$, $x \in f^{-1}(U)$ is equivalent to
$f(x) \in U$. This means that $x \in f^{-1}(U)$ is verifiable: just verify
$f(x) \in U$ instead. In other words, the inverse image $f^{-1}(U)$ is open if
$U$ is open, which is the definition of continuity of~$f$. This is summarized
by our second slogan
% 
\begin{quote}
  \emph{``Computable functions are continuous.''}
\end{quote}

\begin{exercise}
  Show that verifiable properties are closed under finite intersections. Are
  they closed under infinite unions?
\end{exercise}

\section{Compactness}
\label{sec:compactness}

The standard definition of compactness is as follows.

\begin{definition}
  A topological space~$X$ is \emph{compact} when every open cover of~$X$ has a
  finite subcover.
\end{definition}

Our aim is to give the concept of compactness a computational interpretation,
just like we did for open sets and continuous maps above. We first rephrase
this definition in terms of directed open covers. A family $\mathcal{S} =
\set{S_i \such i \in I}$ of sets is \emph{directed} when $I$ is non-empty and
for all $i, j \in I$ there is $k \in I$ such that $S_i \subseteq S_k$ and $S_j
\subseteq S_k$.

\begin{exercise}
  Suppose $\mathcal{S} = \set{S_i \such i \in I}$ is a directed family of
  sets. Show that for any finite number of members $S_{i_1}, \ldots, S_{i_n}
  \in \mathcal{S}$, there is some $S_j \in \mathcal{S}$ which contains them
  all.
\end{exercise}

\begin{exercise} Suppose $\mathcal{S} = \set{S_i \such i \in I}$ is an
  arbitrary family of sets. Show that the family formed by taking all finite
  unions of the $S_i$'s, including the empty one,
  % 
  \begin{equation*}
    \mathcal{T} =
    \set{U_{i_1} \cup \cdots \cup U_{i_n} \such n \geq 0 \land i_1, \ldots, i_n \in I}
  \end{equation*}
  % 
  is directed and that $\bigcup \mathcal{S} = \bigcup \mathcal{T}$.
\end{exercise}


\begin{proposition}
  \label{proposition:compact-directed-cover} A topological space~$X$ is
  \emph{compact} if, and only if, every directed open cover of~$X$ contains
  $X$ as a member.
\end{proposition}

\begin{proof} Suppose $X$ is compact and let $\set{U_i \such i \in I}$ is a
  directed open cover. There exist $i_1, \ldots, i_n \in I$ such that
  $\set{U_{i_1}, \ldots, U_{i_n}}$ already cover~$X$. By directedness there
  exists $j \in I$ such that $U_j$ contains $U_{i_1}, \ldots, U_{i_n}$, but
  then $U_j = X$.

  Conversely, suppose every directed open cover of~$X$ contains $X$ as a
  member, and let $\set{U_i \such i \in U}$ be any open cover of~$X$. We form
  a new cover which consists of all finite unions of $U_i$'s, including the
  empty union:
  % 
  \begin{equation*}
    \set{U_{i_1} \cup \cdots \cup U_{i_n} \such n \geq 0 \land i_1, \ldots, i_n \in I}.
  \end{equation*}
  % 
  This is a directed open cover of~$X$, hence it has a member $U_{i_1} \cup
  \cdots \cup U_{i_n}$ which is equal to~$X$. Thus the original open cover has
  the finite subcover $\set{U_{i_1}, \ldots, U_{i_n}}$.
\end{proof}

Let $X$ be a topological space and $\tpl{X} = \set{U \subseteq X \such
  \text{$U$ is open}}$ its topology. The set $\tpl{X}$ can be considered as a
topological space, too! Say that $\scr{U} \subseteq \tpl{X}$ is \emph{Scott
  open} when
% 
\begin{enumerate}
\item it is an \emph{upper set:} if $U \in \scr{U}$ and $U \subseteq V \in
  \tpl{X}$ then $V \in \scr{U}$,
\item it is \emph{inaccessible by directed families}: if $\set{U_i \such i \in
    I}$ is a directed family of open sets in~$X$ and $\bigcup_{i \in I} U_i
  \in \scr{U}$ then there is $i \in I$ such that $U_i \in \scr{U}$.
\end{enumerate}

\begin{exercise}
  Verify that the family of all Scott open sets forms a topology on $\tpl{X}$.
\end{exercise}

We call the family of all Scott open subsets of~$\tpl{X}$ the \emph{Scott
  topology}. Henceforth, whenever $\tpl{X}$ is viewed as a topological space
we mean the Scott topology.

\begin{proposition}
  \label{proposition:compact-top-open}
  A topological space~$X$ is compact if, and only if, $\set{X}$ is a Scott
  open subset of $\tpl{X}$.
\end{proposition}

\begin{proof}
  This is just Proposition~\ref{proposition:compact-directed-cover} rephrased
  in terms of Scott open sets.
\end{proof}

\begin{exercise}
  Write down enough details of the proof of
  Proposition~\ref{proposition:compact-top-open} to convince yourself that it
  holds.
\end{exercise}

The last proposition allows us to rephrase compactness in terms of verifiable
properties: a space is compact when ``holds for all $x \in X$'' is a
verifiable property of verifiable properties. Admittedly, is a bit confusing,
so we seek another characterization.

A third way to characterize compact spaces involves the \emph{Sierpinski
  space} $\Sigma = \set{\bot, \top}$ which has two points and the open sets
$\emptyset$, $\set{\top}$ and $\set{\bot, \top}$.

\begin{exercise}
  Find all the ways to equip the set $\set{\bot, \top}$ with a topology. How
  many non-homeomorphic ones are there?
\end{exercise}

\begin{exercise}
  Show that $\Sigma$ is homeomorphic to the topological space
  $\tpl{\set{\star}}$, where $\set{\star}$ is a singleton space.
\end{exercise}

\begin{exercise}
  How many maps $\Sigma \to \Sigma$ are there? How many are continuous?
\end{exercise}

\begin{proposition}
  \label{proposition:map-sigma-continuous}
  A map $f : X \to \Sigma$ is continuous if, and only if, $f^{-1}(\set{\top})$
  is open in~$X$.
\end{proposition}

\begin{proof}
  $\Sigma$ only has three open sets. The inverse images $f^{-1}(\emptyset) =
  \emptyset$ and $f^{-1}(\Sigma) = X$ of two of them are always open, which
  leaves us with checking just the third one, $f^{-1}(\set{\top})$.
\end{proof}

The previous proposition tells us that there is a bijection between $\tpl{X}$
and continuous maps $X \to \Sigma$. In one direction the bijection maps an
open subset $U \subseteq X$ to its \emph{characteristic map} $\chi_U : X \to
\Sigma$, defined by
% 
\begin{equation*} \chi_U(x) =
  \begin{cases} \top & \text{if $x \in U$,}\\ \bot & \text{otherwise.}
  \end{cases}
\end{equation*}
% 
In the other direction a continuous map $f : X \to \Sigma$ corresponds to the
open subset $f^{-1}(\set{\top})$.

\begin{proposition}
  \label{proposition:compact-iff-forall-continuous}
  A topological space $X$ is compact if, and only if, the map $\forall_X :
  \tpl{X} \to \Sigma$, defined by
  % 
  \begin{equation*} \forall_X(U) =
    \begin{cases} \top & \text{if $U = X$,}\\ \bot & \text{otherwise}
    \end{cases}
  \end{equation*}
  % 
  is continuous.
\end{proposition}

\begin{proof} By Proposition~\ref{proposition:map-sigma-continuous} continuity
of~$\forall_X$ is equivalent to $\forall_X^{-1}(\set{\top}) = \set{X}$ being
open, which in turn is equivalent to~$X$ being compact by
Proposition~\ref{proposition:compact-top-open}.
\end{proof}

If we turn around the second slogan for a moment,
Proposition~\ref{proposition:compact-iff-forall-continuous} suggests that we
should be able to compute the map $\forall_X$ when $X$ is compact. To explain
what that means precisely, we need to say a few words about how topological
spaces are represented in a programming language.

\section{Representations of spaces}
\label{sec:representations}

A topological space $X$ is an abstract set. In a programming language
it is represented by a suitable datatype $\mathtt{X}$ whose values
represent points of $X$. The relationship between $X$ and $\mathtt{X}$
is expressed as a \emph{realizability} relation $\rz_X$ which relates
values of type~$\mathtt{X}$ to point of $X$. For $\R{r} : \mathtt{X}$
and $a \in X$ we read $\R{r} \rz_X a$ as ``the value $\R{r}$ realizes,
or represents, the point $a$''. We require that each point have at
least one realizer. The triple $(X, \mathtt{X}, {\rz_X})$ is called an
\emph{assembly}. Note that some values of type $\mathtt{X}$ may
represent nothing, and that a point of~$X$ may have many
representations.

A map $f : X \to Y$ is \emph{realized} or represented by a value $\R{f}$ of
type $\mathtt{X} \to \mathtt{Y}$ which \emph{tracks}~$f$:
% 
\begin{equation*}
  \text{if $\R{a} \rz_X a$ then $\R{f}\;\R{a} \rz_Y f(a)$.}
\end{equation*}
%
This is just a formal definition of the informal idea of what it means
to ``implement'' an abstract function between two sets.
 
We adopt the notational convention that datatypes and realizers are written in
typewriter font. For example, if $X$ is the underlying set of an assembly, $a
\in X$, and $f : X \to Y$ a realized map, the corresponding datatype and
realizers are $\mathtt{X}$, $\R{a}$, and $\R{f}$, respectively.

In these notes we want to focus on topology and programming, so we
shall keep assemblies and realized maps in the background as much as
possible. However, it is useful to know that there is a precise
mathematical connection bewtween topology and programming.\footnote{To
  be honest we should point out that realizability is not the only one
  way of relating topology with programming.}


\section{Searchable spaces}
\label{sec:searchable-spaces}

To sensibly talk about computability of the operator $\forall_X : \tpl{X} \to
\Sigma$, we should represent $X$ by a datatype $\mathtt{X}$ (and a
corresponding realizability relation $\rz_X$), and similarly represent
$\Sigma$ by a suitably chosen datatype $\mathtt{S}$. The topology $\tpl{X}$ is
represented by the type $\mathtt{X} \to \mathtt{S}$ because $\tpl{X}$ is in
bijective correspondence with continuous maps $X \to \Sigma$. Hence,
$\forall_X$ is computable if there is a value
% 
\begin{equation*} \R{forall} : (\mathtt{X} \to \mathtt{S}) \to \mathtt{S},
\end{equation*}
% 
which tracks it, i.e., for all $\R{p}$ of type $\mathtt{X} \to \mathtt{S}$ and
all continuous maps $p : X \to \Sigma$,
% 
\begin{equation*} \text{if $\R{p} \rz_{X \to \Sigma} p$ then
$\R{forall}\;\R{p} \rz_\Sigma \forall_X(p)$.}
\end{equation*}
% 
There are several ways of representing $\Sigma$ by a datatype $\mathtt{S}$.
Because all of them are a bit unusual, we first focus on a related but simpler
case
% 
\begin{equation}
  \label{eq:forall-bool} \mathtt{forall} : (\mathtt{X} \to \Bool) \to \Bool.
\end{equation}
% 
The datatype $\Bool$ does \emph{not} represent~$\Sigma$ but the discrete
topological space $\two = \set{0, 1}$. Values of type $\mathtt{X} \to \Bool$
track continuous maps $X \to \two$. Because a map $f : X \to \two$ is
continuous if, and only if, $f^{-1}(\set{1})$ is clopen (closed and open), the
decidable properties correspond to clopen subsets.

\begin{exercise} Show that a map $f : X \to \two$ is continuous if, and only
if, $f^{-1}(\set{1})$ clopen.
\end{exercise}

\begin{exercise} Which properties of $\RR$ are decidable?
\end{exercise}
% 
How could we implement the universal quantifier $\mathtt{forall}$? When $X$ is
finite, say $X = \set{a_1, \ldots, a_n}$ we can simply define
% 
\begin{equation*} \forall_X(p) = p(a_1) \land p(a_2) \land \cdots \land
p(a_n),
\end{equation*}
% 
which is easily implemented.
% 
However, this would not work for infinite sets, where all the fun is. Instead,
we are going to use the notion of a \emph{searchable} space.

\begin{definition} A space $X$ is \emph{searchable} if there is a realized map
$\epsilon : (X \to \two) \to \two$ such that, for all $p : X \to \two$,
  % 
  \begin{equation*} p (\epsilon(p)) \iff \xsome{x}{X}{p(x)}.
  \end{equation*}
\end{definition}

The operator $\epsilon$ takes as input a decidable property $p$ and returns a
candidate $\epsilon(p) \in X$ such that $p(\epsilon(p))$ holds. If there is no
$x \in X$ for which $p(x)$ holds, then $\epsilon(p)$ may be any element of
$X$. We emphasize that $\epsilon(p)$ \emph{must} return a candidate and is not
allowed to diverge. The operator $\epsilon$ is known as \emph{Hilbert's
operator}.

For a searchable space~$X$ the existential quantifer
% 
\begin{equation*} \exists_X(p) = \begin{cases} 1, &\text{if
$\xsome{x}{X}{p(x)}$,}\\ 0, &\text{ otherwise.}
  \end{cases}
\end{equation*}
% 
is easily implemented as
\begin{equation*} \exists_X(p) = p (\epsilon(p)).
\end{equation*}
% 
As an added bonus, because $\xall{x}{X}{p(x)}$ is equivalent to
$\lnot\xsome{x}{X}{\lnot p(x)}$, we also get the universal quantifier
% 
\begin{equation*} \forall_X(p) = \lnot \exists_X(\lnot p).
\end{equation*}
% 
We should give a name to datatypes which have the existential quantifier.

\section{Overt spaces}
\label{sec:overt-spaces}

By symmetry there should be a topological notion that is dual to compactness,
as follows.

\begin{definition} A topological space $X$ is \emph{overt}\footnote{The term
``overt space'' was coined by Paul Taylor.} if the map $\exists_X : \tpl{X}
\to \Sigma$, defined by
  % 
  \begin{equation*} \exists_X (U) =
    \begin{cases} \bot & \text{if $U = \emptyset$,}\\ \top & \text{otherwise.}
    \end{cases}
  \end{equation*}
  % 
  is continuous.
\end{definition}

The following proposition is rather disappointing.

\begin{proposition} Every topological space is overt.
\end{proposition}

\begin{proof} By Proposition~\ref{proposition:map-sigma-continuous} we only
need to check that $\exists_X^{-1}(\set{\top}) = \tpl{X} \setminus
\set{\emptyset}$ is Scott open, which is easy. Clearly $\tpl{X} \setminus
\set{\emptyset}$ is an upper set, and if $\bigcup_{i \in I} U_i$ is non-empty
then at least one member $U_i$ must be non-empty.
\end{proof}

It would be \emph{wrong} to dismiss the notion of overtness just because it is
trivial in classical topology. As we shall see later, in computable topology
overtness is not only interesting but just as fundamental as compactness. We
now return to searchable spaces and their implementation.

\section{Implementation of searchable spaces}
\label{sec:implementation-searchable}

The code from this section is based on that of Martín Escardó's blog
post~\cite{escardo08:blog}. A searchable space is represented by a datatype
$\mathtt{X}$ and the $\epsilon$ operator, which we shall call $\R{epsilon}$ in
code. In Haskell we would define it like this:
% 
\begin{lstlisting}
data Searchable a = Finder ((a -> Bool) -> a)
\end{lstlisting}
% 
The first line defines a datatype $\mathtt{Searchable}\;\alpha$, where
$\alpha$ is a type parameter. An element of this datatype is of the
form $\mathtt{Finder}\;\epsilon$, where $\epsilon$ is a search
operator. Think of an element $\R{s}$ of type
$\mathtt{Searachable}\;\alpha$ as a searchable subspace of the
datatype~$\alpha$. The subspace is given in terms of its search
operator.

We define an auxiliary function $\mathtt{find}$, so that we can
conveniently write $\R{find}\;\R{s}\R{p}$ and read it ``search in
space $\R{s}$ for a candidate satisfying $\R{p}$'':
%
\begin{lstlisting}
find :: Searchable a -> (a -> Bool) -> a
find (Finder epsilon) p = epsilon p
\end{lstlisting}
%
The function $\mathtt{find}$ always finds a candidate, which may or
may not satisfy the given condition. It is convenient have another
function $\mathtt{search}$ which returns $\mathtt{Nothing}$ or
$\mathtt{Just}\;x$ depending on whether there is an $x$ satisfying the
given condition:
% 
\begin{lstlisting}
search :: Searchable a -> (a -> Bool) -> Maybe a
search s p =
    let x = find s p
    in if p x then Just x else Nothing
\end{lstlisting}
% 
And here are the quantifiers:
% 
\begin{lstlisting}
exists s p = p (find s p)
forall s p = not (exists s (not . p))
\end{lstlisting}
% 
Now we actually define some searchable spaces. The easiest is the singleton
space $\set{x}$ where the only candidate is $x$:
% 
\begin{lstlisting}
singleton x = Finder (\p -> x)
\end{lstlisting}
% 
In Haskell the notation \texttt{\char92 x -> e} means ``the function which
maps $x$ to $e$''. Thus we read the definition above as ``the constant
function which maps every $p$ to $x$''.
% 
For a space with two points $\set{x, y}$ the $\epsilon$ operator needs to
check first whether $p(x)$ holds:
% 
\begin{lstlisting}
doubleton x y = Finder (\p -> if p x then x else y)
\end{lstlisting}
% 
The generalization of singletons and doubletons are finite sets. Given a list
of elements $[x_1, \ldots, x_n]$, we construct the searchable space $\set{x_1,
  \ldots, x_n}$:
% 
\begin{lstlisting}
finite_set :: [a] -> Searchable a

finite_set lst = Finder (\p ->
    let loop []     = undefined
        loop [x]    = x
        loop (x:xs) = if p x then x else loop xs
    in loop lst)
\end{lstlisting}
% 
Note that \texttt{finite\_set\;[]} is undefined because the empty set is not
searchable.
% 
There are many other constructions of searchable spaces, such as the disjoint
sum:\footnote{In Haskell the disjoint sum $X + Y$ is the datatype
  $\mathtt{Either}\;X\;Y$, and the canonical inclusions are called
  $\mathtt{Left}$ and $\mathtt{Right}$, respectively.}
% 
\begin{lstlisting}
sum s t = Finder (\p -> let x = Left (find s (p . Left))
                            y = Right (find t (p . Right))
                        in if p x then x else y)
\end{lstlisting}
% 
A more complicated operation is a union of a searchable family of
searchable (sub)spaces. Suppose $I$ is a searchable space and
${S_i}_{i \in I}$ a family of searchable subspaces, $S_i \subseteq X$.
Then their union $S = \bigcup_{i \in I} S_i$ is also searchable, with
the search operator defined as follows:
%
\begin{equation*}
  \epsilon_S(p) = \text[not finished]
\end{equation*}
%
To get things organized, we shall define a \emph{search monad} which
will significantly simplify the code we have to write.\footnote{If you
  are not familiar with Haskell monads, you should now read
  Appendix~\ref{app:monads}.}


% BIBLIOGRAPHY (fold)

\bibliographystyle{plain}
\bibliography{../realizability,../cca,../notes}

% bibliogprahy (end)
\appendix
    
\section{Haskell monads by examples} % (fold)
\label{app:monads}

There are any number of introductions on monads written by Haskell bloggers
around the internet. You may want to look at some of them for further insight.
We shall introduce monads by examples.

Haskell is a purely functional language. In particular, it does not allow
direct invocation of any operations that would allow us to detect the order of
evaluation. This rules out mutable variables, exceptions, I/O, and many other
useful programming concepts. All these can be put back into Haskell with a
clever way of programming known as \emph{monadic style}.

Our first example is a very simple kind of exception which is sometimes known
as \emph{abort}. It is an operation which aborts whatever is being computed
and cannot be intercepted. To have something like that in Haskell, we need to
be explicit about values which may trigger abort. So we define a datatype
$\mathtt{Abortable}\;t$ which means ``either an ordinary value of type $t$, or
a special value $\mathtt{Aborted}$ indicating that abort
happened'':\footnote{the $\mathtt{deriving}$ clause is of no concern right
  now, it means Haskell will automatically derive a way of showing abortable
  values on screen}

\begin{lstlisting}
data Abortable t = Value t | Aborted
                   deriving Show  
\end{lstlisting}
% 
The constant $\mathtt{Aborted}$ signifies a value whose computation was
aborted. The other possibility is a value of the form $\mathtt{Value}\;v$,
which signifies a successfully computed value $v$ (abort did not happen).
Haskell insists that we write $\mathtt{Value}\;v$ rather than just $v$. This
way it can tell the difference between ordinary values and values that could
have been aborted but were not.

An ordinary value $v$ of type $t$ may always be converted to an abortable
value $\mathtt{Value}\;v$ of type $\mathtt{Abortable}\;t$. The operation that
does this is called $\mathtt{return}$ in Haskell and is the first half of a
monad. The second half of a monad is an operation \cc{>>=}, called
\emph{bind}, which combines an abortable value
% 
\begin{lstlisting}
x :: Abortable
\end{lstlisting}
% 
and a function
\begin{lstlisting}
f :: a -> Abortable b  
\end{lstlisting}
% 
which expects an ordinary value and outputs an abortable one. This is written
as
% 
\begin{lstlisting}
x >>= f
\end{lstlisting}
% 
What should \cc{x >>= f be}? Well, if \cc{x} is \cc{Aborted} then \cc{x >>=} f
must also be \cc{Aborted} (we want \cc{Aborted} to act as an uncatchable
exception). If \cc{x} is of the form \cc{Value v} then \cc{x >>= f} should be
\cc{f v}. This brings us to the official monad definition:
% 
\begin{lstlisting}
instance Monad Abortable where
   return v        = Value v
   Aborted >>= f   = Aborted
   (Value v) >>= f = f v
\end{lstlisting}
% 
In principle we can use \cc{return} and \cc{>>=} to compute with abortable
values, but it is very cumbersome. For example, suppose we have a division
operation whose result is an abortable integer,
% 
\begin{lstlisting}
divide :: Int -> Int -> Abortable Int
divide x 0 = Aborted
divide x y = Value (x `div` y)  
\end{lstlisting}
% 
In order to compute the function which maps $x$ and $y$ to $x/y + y/x$ we have
to write
% 
\begin{lstlisting}
f :: Int -> Int -> Abortable Int
f x y = (divide x y) >>= (\u -> (divide y x) >>= (\v -> return (u + v)))
\end{lstlisting}
% 
With good indentation it is possible to improve the code a bit:
% 
\begin{lstlisting}
g :: Int -> Int -> Abortable Int
g x y = (divide x y) >>= (\u ->
        (divide y x) >>= (\v ->
        return (u + v)))
\end{lstlisting}
% 
This we can read as: feed the abortable value \cc{divide x y} into \cc{u},
feed the abortable value \cc{divide y x} into \cc{v}, then \cc{return} the
abortable value \cc{u+v}. The operator \cc{>>=} makes sure that the whole
result is \cc{Aborted} if either \cc{u} or \cc{v} is. Haskell has special
notation which significantly improves the code:
% 
\begin{lstlisting}
h :: Int -> Int -> Abortable Int
h x y = do u <- divide x y
           v <- divide y x
           return (u + v)
\end{lstlisting}
% 
The functions \cc{g} and \cc{h} are the exact same thing written in different
notations.

The \cc{Abortable} monad is already built into Haskell, except it is called
the \cc{Maybe} monad. Instead of \cc{Value v} and \cc{Aborted} it has \cc{Just
  v} and \cc{Nothing}. To give an example of its use, suppose we have an
association list
% 
\begin{lstlisting}
lst = [("apple", 3), ("orange", 10), ("banana", 2), ("stone", 6)]
\end{lstlisting}
% 
and would like to find the value corresponding to \cc{"orange"}. We can do
this by writing
% 
\begin{lstlisting}
y = lookup "orange" lst -- y equals Just 10
\end{lstlisting}
% 
The answer is \cc{Just 10}. If we lookup something that is not in the list we
get \cc{Nothing}:
% 
\begin{lstlisting}
  z = lookup "cow" lst -- z equals Nothing
\end{lstlisting}
% 
The monad and do notation come in handy if we have a piece of code that does
several lookups and we want it to fail as soon as one of the lookups fails,
e.g.
% 
\begin{lstlisting}
sum = do u <- lookup "banana" lst
         v <- lookup "apple" lst
         w <- lookup "cherry" lst
         return (u + v + w)
\end{lstlisting}
% 
The value of \cc{sum} is \cc{Nothing} because the third lookup fails. Had it
succeeded, we would get \cc{Just i} for some integer \cc{i}. Here is one way
of writing the same code without the do notation:
% 
\begin{lstlisting}
sum' = case (lookup "banana" lst, lookup "apple" lst, lookup "cherry" lst) of
          (Just u, Just v, Just w) -> Just (u + v + w)
          (_, _, _) -> Nothing
\end{lstlisting}
% 
You may judge for yourself which one is more readable.

The next example of a monad is non-deterministic choice operator. Suppose in a
computation we have choice points, at which we can choose any value from a
given list. Such a situation occurs when we perform a search over a tree, and
at each node we can choose one of the branches. We would like a convenient way
of programming choice points so that when we evaluate an expression with
choice points all possible combinations of choices are taken, and the possible
results are stored in a list.

First we define a datatype \cc{Choose t} which holds possible results of a
computation of type \cc{t} with choice points:
% 
\begin{lstlisting}
data Choose t = Choices [t]
                deriving Show
\end{lstlisting}
% 
For example, the value \cc{Choices [1,2,3]} means that the possible outcomes
are $1$, $2$, and $3$. To get a monad, we must define return, which converts
an ordinary value \cc{v} of type \cc{t} to one of type \cc{Choose t}. This
part is more or less obvious as we simply return \cc{Choices [v]}.
% 
To define \cc{>>=} we have to think about how to combine a value
$\mathtt{Choices}\;[v, \ldots, v_n]$ with a function $f$ which accepts an
ordinary value v and returns some choices. It is not hard to see that we
should loop over $v_1, ..., v_n$, get all the different choices produced by
$f$ and combine them into a single choice. The end result is the following
monad
% 
\begin{lstlisting}
instance Monad Choose where
     return v = Choices [v]
     (Choices lst) >>= f = Choices (combine f lst [])
         where combine f [] ws     = ws
               combine f (v:vs) ws = let Choices us = f v
                                     in combine f vs (ws ++ us)
\end{lstlisting}
% 
Now we can use the do notation to write programs like this:
% 
\begin{lstlisting}
-- all sums of the form x+y where x is 1,...,10 and y is 1,...,x.
c = do x <- [1..10]
       y <- [1..x]
       return (x + y)
-- c is Choices [2,3,4,4,5, ..., 18,19,20] (55 elements)
\end{lstlisting}
% 
Without the do notation we would have to have a double loop of some kind. The
monad we just considered is also built into Haskell, and is known as the
\emph{list monad}. Possible choices are represented as a list, so we can write
$[x_1, \ldots, x_n]$ rather than $\mathtt{Choices}\; [x_1, \ldots, x_n]$:
% 
\begin{lstlisting}
-- all sums of the form x+y where x is 1,...,10 and y is 1,...,x.
d = do x <- [1..10]
       y <- [1..x]
       return (x + y)
-- d is [2,3,4,4,5, ..., 18,19,20] (55 elements)
\end{lstlisting}

Our last example is the state monad. In procedural programming languages we
have variables that can be updated, i.e., their values change as computation
progresses. We say that the computation is \emph{stateful} because it depends
on the state of the variables, and it may change the state of the variable.

In general, stateful computation is a function which accepts the current state
and returns a result together with the new state. That is, in Haskell a
stateful computation computing a result of type \cc{t} and having access to
state of type \cc{s} is a function \cc{s -> (t, s)} which accepts the current
state \cc{m}, and returns a pair \cc{(x,m')} representing the computed value
\cc{x} and the new state \cc{m'}.
% 
We define a Haskell type of such stateful computations:
% 
\begin{lstlisting}
data State s t = Stateful (s -> (t, s))  
\end{lstlisting}
% 
Here is an example of a stateful computation:
% 
\begin{lstlisting}
incr = Stateful $ \m -> (m, m+1)  
\end{lstlisting}
% $
By the way, the mysterious \$ does nothing, except it allows us to write fewer
parenthesis. The meaning of \cc{f \$ x} is the same as \cc{f x}, except that
\cc{\$} is an infix operator with low precedence. If we wrote \cc{incr}
without \$, we would have to write \cc{Stateful (\m -> (m, m+1))}.

The definition of \cc{incr} is read as follows: \cc{incr} is a stateful
computation which takes a memory location \cc{m} and returns the current value
of \cc{m}. It also increases the memory location by \cc{1}. In other words,
this would be written as \cc{m++} in C.

But how do we actually execute \cc{incr}? We give it the initial value of the
memory location and out comes the result together with updated memory:
% 
\begin{lstlisting}
(r1,m1) = let Stateful v = incr in v 42 -- r1 is 42, m1 is 44
\end{lstlisting}
% 
This is quite ugly, so we define an auxiliary function
% 
\begin{lstlisting}
run m (Stateful v) = v m
\end{lstlisting}
% 
Now we can write
% 
\begin{lstlisting}
(r2, m2) = run 42 incr -- r2 is 42, m2 is 44
\end{lstlisting}
% 
and read it as ``run \cc{incr} with initial state 42''. Let us also define the state monad:
% 
\begin{lstlisting}
instance Monad (State s) where
     return x = Stateful $ \m -> (x, m)
     v >>= f  = Stateful $ \m -> let (x, m') = run m v in run m' (f x)
\end{lstlisting}
% $
Return converts a pure (non-stateful) value to a stateful one that doesn't change the state:
The operation \cc{>>=} chains together stateful computations:
% 
To compute \c{v >>= f} in state \c{m} we first run \c{v} in state \c{m} to
obtain a result \c{x} and the new state \c{m'}. We then compute \c{f x} in
state \c{m'}. This is all good and well, but the do notation does not allow us
to manipulate the state, so we also need basic operations that
do:\footnote{The Haskell value \cc{()} is the empty tuple, the equivalent of
  \cc{void} in C++ and Java.}
% 
\begin{lstlisting}
get = Stateful $ \m -> (m, m)        -- get the current value of state
update n = Stateful $ \m -> ((), n)  -- update state to n, give () as result
\end{lstlisting}
% 
With this we may write procedural program:
% 
\begin{lstlisting}
a = run 42 (do x <- get        -- let x be current value of memory (42)
               update (x+1)    -- update memory to x+1 (43)
               y <- get        -- let y be current value of memory (43)
               update 7        -- update memory to 7
               return (x + y)) -- return x+y (42+43 = 85)
-- a is (85, 7)  
\end{lstlisting}
% 
Let us conclude this short introduction by giving the official equations that
\cc{return} and \cc{>>=} must satisfy in order to deserve the name
\emph{monad}:
% 
\begin{align*}
  \mathtt{return}\; a \bind k &= k\; a \\
  m \bind \mathtt{return}              &=   m \\
  m \bind (\backslash x -> k x \bind h)   &=   (m \bind k) \bind h
\end{align*}
% 
Further reading:
% 
\begin{enumerate}
\item  \url{http://www.haskell.org/tutorial/monads.html}:
  ``A Gentle Introduction to Haskell'', Chapter 9: About Monads
\item \url{http://www.haskell.org/all_about_monads/html/index.html}
  ``All about Monads: 
  A comprehensive guide to the theory and practice of monadic programming in Haskell''
\item \url{http://blog.sigfpe.com/}
  ``A neighborhood of infinity''.
  Dan Piponi's blog has many clever and mind boggling examples of monads.
\end{enumerate}
% section haskell_monads_by_examples (end)
\end{document}
